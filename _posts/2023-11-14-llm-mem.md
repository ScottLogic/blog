---
title: 'LLM finetuning: memory requirements, expectation vs reality'
date: 2023-11-14 00:00:00 Z
categories:
- abirch
- Tech
author: abirch
layout: default_post
summary: The memory costs for LLM training are large but predictable.
---

<style>
  table.lm-lefthead tbody th {
    text-align: left;
  }
  table.lm-greyhead tbody th {
    background-color: #f8f8f8;
  }
  table.lm-wireframe tbody tr {
    background-color: initial;
  }
  table.lm-wireframe th, table.lm-wireframe td {
    border: 1px solid;
  }
  details {
    font-weight: 300;
  }
  summary {
    font-weight: 300;
    display: block;
  }
  summary::after {
    cursor: pointer;
    content: '[+more]';
    text-decoration: underline;
    text-decoration-style: dotted;
    padding-left: 0.5em;
    font-size: 0.8em;
  }
  details[open] > summary::after {
    content: ' [−less]';
  }
</style>

If you want to try your hand at fine-tuning an LLM (Large Language Model): one of the first things you’re going to need to know is "will it fit on my GPU".

The pre-eminent guide to estimating (VRAM) memory requirements is [Transformer Math 101](https://blog.eleuther.ai/transformer-math/). It bears mentioning, though, that its heuristics are written in the context of frameworks such as GPT-NeoX and Megatron-DeepSpeed. These are specialist frameworks for training at scale, and are not what a beginner would reach for.

Are Transformer Math’s memory estimates applicable to idiomatic PyTorch, without external frameworks? Let’s put it to the test.

## Experimental setup

We built a simple training loop ([`mem_llm.py`](https://github.com/scottlogic-alex/qlora/blob/memory-investigation/mem_llm.py)). It loads a model via HF (HuggingFace) transformers, and trains said model to complete random sequences.
By running this training loop for a few steps, then taking a measurement: we can determine our peak memory usage.

We will evaluate the cost of three common techniques provided by an ML accelerator (explanations to follow later):

- [Mixed-precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) computation
- [DDP](https://arxiv.org/abs/2006.15704) (Distributed Data Parallel) parallelism
- [Gradient accumulation](https://medium.com/mlearning-ai/gradient-accumulation-307de7599e87)

 <!-- The <abbr title="World Health Organization">WHO</abbr> was founded in 1948.  -->

In our trainer: this functionality is built using PyTorch idioms. Our implementation is similar to how <abbr title="HuggingFace">HF</abbr> Accelerate operates in its most basic configurations (i.e. when it is not configured to delegate to other accelerators such as DeepSpeed or Megatron).

## What we expect to see

Before we dive into measurements: let’s estimate how much memory we’ll need in theory. We will use reasoning similar to Transformer Math.

For _non-distributed_ training,  
```
Total Training Memory = Model + Optimiser + Activations + Gradients
```

Our experimental setup **eliminates optimiser costs**, through means of a _stateless optimizer_ (SGD without momentum).

<p>
<details><summary>We also <strong>minimise activations</strong>, by using <em>tiny batch sizes</em> and <em>sequence lengths</em>.</summary>At batch-of-1, sequence length 8: we expect activations to be negligible (<100MiB) even on our largest model (Llama 7b), according to Transformer Math's worst-case activations formula, "no recomputation" (i.e. gradient checkpointing disabled).</details>
</p>

These measures narrow down the possible sources of overhead, if detected.

### Estimated baseline costs in single-precision

First up, let's establish the terminology we're going to use:

<table class="lm-wireframe lm-lefthead lm-greyhead">
  <thead>
    <th>Precision</th>
    <th>Datatype(s)</th>
    <th>Bytes per element</th>
  </thead>
  <tbody>
    <tr>
      <td>Single</td>
      <td>
        <code class="highlighter-rouge">float32</code>
      </td>
      <td>4</td>
    </tr>
    <tr>
      <td>Half</td>
      <td>
        <code class="highlighter-rouge">float16</code>,
        <code class="highlighter-rouge">bfloat16</code>
      </td>
      <td>2</td>
    </tr>
  </tbody>
</table>

The memory costs for pure-float32 training are relatively straightforward:

<table class="lm-wireframe lm-lefthead lm-greyhead">
  <tbody>
    <tr>
      <th rowspan="2">Model</th>
      <th>Params</th>
      <td>4 bytes/param</td>
    </tr>
    <tr>
      <th>Gradients</th>
      <td>4 bytes/param</td>
    </tr>
    <tr>
      <th colspan="2">Optimizer states</th>
      <td>None in our setup*</td>
    </tr>
    <tr>
      <th colspan="2">Activations</th>
      <td>Negligible in our setup**</td>
    </tr>
  </tbody>
  <tfoot>
    <tr>
      <th colspan="2">Total</th>
      <td>8 bytes/param</td>
    </tr>
  </tfoot>
</table>

<p>
<details><summary>* It's worth noting that our use of a stateless optimiser is highly unrealistic; typical optimiser costs are <em>substantial</em>.</summary>
Enlisting an uncontroversial optimiser such as 32-bit AdamW would cost an additional 8 bytes/param.</details>
<details><summary>** Likewise, activations would <em>normally</em> be a substantial cost, easily capable of growing bigger than all other costs combined.</summary>
Even more so in pure-float32 training, where our activations will be 32-bit. By comparison: mixed-precision training enables activations to be smaller (employing the smaller datatype used for reduced-precision compute).</details>
</p>

We don’t dwell on this baseline, because it is a bit of a strawman. Floating-point operations are slow in 32-bit; they don’t utilize tensor cores, and lack accelerated kernels for essential operations such as Flash Attention.

For these reasons, and to reduce the cost of activations: it is standard to train in mixed-precision.

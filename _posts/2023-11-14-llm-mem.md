---
title: 'LLM finetuning: memory requirements, expectation vs reality'
date: 2023-11-14 00:00:00 Z
categories:
- Artificial Intelligence
tags:
- artificial intelligence
- ai
- generative AI
- GenAI
- Large Language Models
- LLM
author: abirch
summary: The memory costs for LLM training are large but predictable.
---

<style>
  table.lm-lefthead tbody th {
    text-align: left;
  }
  table.lm-greyhead tbody th {
    background-color: #f8f8f8;
  }
  table.lm-wireframe tbody tr {
    background-color: initial;
  }
  table.lm-wireframe th, table.lm-wireframe td {
    border: 1px solid;
  }
  table.lm-dense th {
    font-size: 0.8em;
    font-weight: 500;
  }
  table.lm-dense th, table.lm-dense td, table.lm-dense tr:first-child td {
    padding-top: 2px;
    padding-bottom: 3px;
    padding-right: 5px;
    padding-left: 5px;
  }
  table.lm-prose-dense th, table.lm-prose-dense td {
    font-size: 0.9em;
  }
  table.lm-prose-dense th {
    font-weight: 500;
  }
  table.lm-prose-dense th, table.lm-prose-dense td, table.lm-prose-dense tr:first-child td {
    padding-top: 5px;
    padding-bottom: 6px;
    padding-right: 8px;
    padding-left: 8px;
  }
  details {
    font-weight: 300;
  }
  summary {
    font-weight: 300;
    display: block;
  }
  summary::after {
    cursor: pointer;
    content: '[+more]';
    text-decoration: underline;
    text-decoration-style: dotted;
    padding-left: 0.5em;
    font-size: 0.8em;
  }
  details[open] > summary::after {
    content: ' [−less]';
  }
  .default-post .post-content a.text-ref {
    text-decoration: underline;
    text-decoration-style: dotted;
    text-decoration-thickness: 1px;
    text-decoration-color: black;
    border-bottom: initial;
  }
  .reset-fontweight {
    font-weight: 300;
  }
  .right {
    text-align: right;
  }
  .center {
    text-align: center;
  }
  .off {
    background-color: #ffcccc;
    text-align: center;
  }
  .on {
    background-color: #d9ead3;
    text-align: center;
  }
  .datum {
    font-family: monospace;
    text-align: right;
  }
  .null {
    background-color: #f3f3f3;
  }
  .oom {
    font-family: monospace;
    text-align: right;
    background-color: #dd7e6b;
  }
  .bottom {
    vertical-align: bottom;
  }
</style>

If you want to try your hand at fine-tuning an LLM (Large Language Model): one of the first things you’re going to need to know is "will it fit on my GPU".

The pre-eminent guide to estimating (VRAM) memory requirements is [Transformer Math 101](https://blog.eleuther.ai/transformer-math/). It bears mentioning, though, that its heuristics are written in the context of frameworks such as GPT-NeoX and Megatron-DeepSpeed. These are specialist frameworks for training at scale, and are not what a beginner would reach for.

Are Transformer Math’s memory estimates applicable to idiomatic PyTorch, without additional frameworks? We put it to the test.

In brief, we found:

- Memory costs measured on small models extrapolate well to larger models
- PyTorch <abbr title="Automated Mixed Precision">AMP</abbr> costs are similar to the frameworks discussed in Transformer Math, but (see next)
- DeepSpeed ZeRO's fused optimiser saves 2 bytes/param on mixed-precision over PyTorch <abbr title="Automated Mixed Precision">AMP</abbr>
- Gradient accumulation can be ~free, but costs more if used in concert with <abbr title="Automated Mixed Precision">AMP</abbr> or <abbr title="Distributed Data Parallel">DDP</abbr>
- <abbr title="Automated Mixed Precision">AMP</abbr> usually costs ~2 bytes/param. Cost increases when gradient accumulation is enabled, or becomes ~free if used in concert with <abbr title="Distributed Data Parallel">DDP</abbr>
- <abbr title="Distributed Data Parallel">DDP</abbr> usually costs ~4 bytes/param, but becomes cheaper if used in concert with <abbr title="Automated Mixed Precision">AMP</abbr>
- <abbr title="Distributed Data Parallel">DDP</abbr> can be made 2.5 bytes/param cheaper by enabling <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"><code>gradient_as_bucket_view</code></a>
- Only the smallest LLMs can be trained under <abbr title="Distributed Data Parallel">DDP</abbr> with realistic settings, even on 80GiB GPUs.
- Training large LLMs requires more advanced distributed training techniques, to shard memory costs over multiple GPUs
- <a href="https://github.com/huggingface/peft">Parameter-efficient finetuning</a> is a more realistic option for the GPU-poor

Read on to see our reasoning and evidence.

<details><summary><em>Click</em></summary>
<em>(yes, like that)</em>
</details>
<p>
<em>to reveal more detail. We hide deep-dives in these collapsible sections to keep the article snappy.</em>
</p>

## Experimental setup

We built a simple training loop ([`mem_llm.py`](https://github.com/scottlogic-alex/qlora/blob/memory-investigation/mem_llm.py)). It loads a model via HF (HuggingFace) [transformers](https://github.com/huggingface/transformers/), and trains said model to complete random sequences.
By running this training loop for a few steps, then taking a measurement: we can determine our peak memory usage.

We will evaluate the cost of three common techniques provided by an ML accelerator (explanations to follow later):

- [Mixed-precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) computation
- [DDP](https://arxiv.org/abs/2006.15704) (Distributed Data Parallel) parallelism
- [Gradient accumulation](https://medium.com/mlearning-ai/gradient-accumulation-307de7599e87)

In our trainer: this functionality is built using PyTorch idioms. Our implementation is similar to how <abbr title="HuggingFace">HF</abbr> [accelerate](https://github.com/huggingface/accelerate/) operates in its most basic configurations (i.e. when it is not configured to delegate to other accelerators such as DeepSpeed or Megatron).

## What we expect to see

Before we dive into measurements: let’s estimate how much memory we’ll need in theory. We will use reasoning similar to Transformer Math.

For _non-distributed_ training,  
```
Total Training Memory = Model + Optimiser + Activations + Gradients
```

<p id="no-optimiser-cost">
Our experimental setup <strong>eliminates optimiser costs</strong>, through means of a <em>stateless optimiser</em> (SGD without momentum).
</p>

<p id="low-activation-cost">
<details><summary>We also <strong>minimise activations</strong>, by using <em>tiny batch sizes</em> and <em>sequence lengths</em>.</summary>At batch-of-1, sequence length 8: we expect activations to be negligible (<100MiB) even on our largest model (Llama 7b), according to Transformer Math's worst-case activations formula, "no recomputation" (i.e. gradient checkpointing disabled).</details>
</p>

These measures narrow down the possible sources of overhead, if detected.

### Estimated baseline costs in single-precision

First up, let's establish the terminology we're going to use:

<table class="lm-wireframe lm-lefthead lm-greyhead lm-prose-dense">
  <thead>
    <th>Precision</th>
    <th>Datatype(s)</th>
    <th>Bytes per element</th>
  </thead>
  <tbody>
    <tr>
      <td>Single</td>
      <td>
        <code class="highlighter-rouge">float32</code> (may imply <a href="https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/"><code class="highlighter-rouge">tf32</code></a>)
      </td>
      <td>4</td>
    </tr>
    <tr>
      <td>Half</td>
      <td>
        <code class="highlighter-rouge">float16</code>,
        <code class="highlighter-rouge">bfloat16</code>
      </td>
      <td>2</td>
    </tr>
  </tbody>
</table>

The memory costs for pure-float32 training are relatively straightforward:

<table class="lm-wireframe lm-lefthead lm-greyhead lm-prose-dense">
  <tbody>
    <tr>
      <th rowspan="2">Model</th>
      <th>Params</th>
      <td>4 bytes/param</td>
    </tr>
    <tr>
      <th>Gradients</th>
      <td>4 bytes/param</td>
    </tr>
    <tr>
      <th colspan="2">Optimiser states</th>
      <td><a class="text-ref" href="#no-optimiser-cost">None</a> in our setup*</td>
    </tr>
    <tr>
      <th colspan="2">Activations</th>
      <td><a class="text-ref" href="#low-activation-cost">Negligible</a> in our setup**</td>
    </tr>
  </tbody>
  <tfoot>
    <tr>
      <th colspan="2">Total</th>
      <td>8 bytes/param</td>
    </tr>
  </tfoot>
</table>

<p>
<details><summary>* It's worth noting that our use of a stateless optimiser is highly unrealistic; typical optimiser costs are <em>substantial</em>.</summary>
Enlisting an uncontroversial optimiser such as 32-bit AdamW would cost an additional 8 bytes/param.</details>
<details><summary>** Likewise, activations would <em>normally</em> be a substantial cost, easily capable of growing bigger than all other costs combined.</summary>
Even more so in pure-float32 training, where our activations will be 32-bit. By comparison: mixed-precision training enables activations to be smaller (employing the smaller datatype used for reduced-precision compute).</details>
</p>

<p>
<details><summary>We don’t dwell on this baseline, because it is a bit of a strawman. Floating-point operations are <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">slow in 32-bit</a>.</summary>
This can be alleviated somewhat via the <a href="https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/">TensorFloat-32</a> optimization, which enables some operations to utilize tensor cores . But ultimately, wider formats incur more I/O costs, and lack accelerated kernels for essential operations such as <a href="https://arxiv.org/abs/2205.14135">Flash Attention</a>.</details>
</p>

For performance reasons, and to reduce the cost of activations: it is standard to train in mixed-precision.

### Estimated baseline costs in mixed-precision

Mixed-precision costs are framework-specific. We will interpret the costs for the frameworks considered in Transformer Math, and also make an educated guess at the costs of PyTorch’s built-in mixed-precision implementation, <abbr title="Automated Mixed Precision">AMP</abbr>.

Transformer Math was validated against GPT-NeoX (circa April 2023), and was cross-checked against Megatron-DeepSpeed (against which its predictions held, for everything except activation checkpointing).  
Its estimates are expected to be predictive to within ±10%, including on other accelerators such as Megatron-LM, PyTorch FSDP, and ColossalAI.  
_Thanks go to Quentin Anthony at EleutherAI for confirming the above._

GPT-NeoX implements mixed-precision via DeepSpeed ZeRO. In both DeepSpeed ZeRO and Megatron-DS: the model is held in half-precision, whilst the optimiser takes a float32 copy of the parameters.

PyTorch AMP (Automated Mixed Precision) implements mixed-precision moreorless the opposite way around. The model is held in float32, and <abbr title="Automated Mixed Precision">AMP</abbr> manages a half-precision copy.

Our understanding is as follows:

<table class="lm-wireframe lm-lefthead lm-greyhead lm-prose-dense">
  <thead>
    <tr>
      <th colspan="2"/>
      <th><abbr title="Megatron-DeepSpeed">Megatron-DS</abbr>, <abbr title="DeepSpeed">DS</abbr> ZeRO</th>
      <th>Pytorch <abbr title="Automated Mixed Precision">AMP</abbr></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2">Model</th>
      <th>Params</th>
      <td>2 bytes/param</td>
      <td>4 bytes/param</td>
    </tr>
    <tr>
      <th>Gradients</th>
      <td>2 bytes/param</td>
      <td>4 bytes/param</td>
    </tr>
    <tr>
      <th rowspan="2">Compute Copy</th>
      <th>Params</th>
      <td rowspan="2">None</td>
      <td>2 bytes/param*</td>
    </tr>
    <tr>
      <th>Gradients</th>
      <td>0 bytes/param**</td>
    </tr>
    <tr>
      <th rowspan="2">Optimiser Copy</th>
      <th>Params</th>
      <td>4 bytes/param</td>
      <td rowspan="2">None</td>
    </tr>
    <tr>
      <th>Gradients</th>
      <td>4 bytes/param*** <abbr title="if (and only if)">iff</abbr> unused</td>
    </tr>
    <tr>
      <th colspan="2">Optimiser states</th>
      <td colspan="2"><a class="text-ref" href="#no-optimiser-cost">None</a> in our setup</td>
    </tr>
    <tr>
      <th colspan="2">Activations</th>
      <td colspan="2"><a class="text-ref" href="#low-activation-cost">Negligible</a> in our setup</td>
    </tr>
  </tbody>
  <tfoot>
    <tr>
      <th colspan="2">Total</th>
      <td>
        8 bytes/param (fused), or<br>
        12 bytes/param (unfused)
      </td>
      <td>10 bytes/param</td>
    </tr>
  </tfoot>
</table>

<p>
<details><summary>*We believe that a compute copy is not taken for parameters whose layers <a href="https://pytorch.org/docs/stable/amp.html#cuda-ops-that-can-autocast-to-float32">autocast to single-precision</a></summary> (for example Embedding and LayerNorm – investigated in this <a href="https://github.com/scottlogic-alex/qlora/blob/memory-investigation/investigate_layernorm.py">minimal reproducer</a>).<br>
LayerNorm params are insignificant (comprising 0.00% of llama-7b’s params), but Embedding can be one of the biggest layers in an LLM. Embedding becomes a smaller proportion of parameters as a model is scaled up, so we can disregard it as far as the overall pattern is concerned.<br>
Consider this an upper bound, because the lifetime of a compute copy is a performance choice: it’s needed during both the forward and backward passes, so the fastest approach is to set aside enough memory to maintain permanent compute copies for all layers. But at minimum: one layer at a time can be recreated from the model’s float32 parameters, then freed after computation (and this work can be re-done during the backwards pass).</details>
<details><summary>**In PyTorch <abbr title="Automated Mixed Precision">AMP</abbr>: we believe the half-precision gradients are never materialized.</summary> They are instead reduced into the model’s float32 gradients immediately. Looking at <a href="https://gist.github.com/scottlogic-alex/9796d5583e5fe7672d7d829734c84c81">memory snapshots</a>: we see that the backwards pass for a Linear layer allocates a float32-sized buffer but no corresponding half-precision-sized buffer.</details>
<details><summary>***Transformer Math does not mention a "4 bytes/param master gradients" cost. We think this assumes a fused optimiser, such as DeepSpeed provides.</summary> We believe we see evidence of a master gradient cost <a href="https://github.com/microsoft/Megatron-DeepSpeed/blob/8e1ff895b5f646f2dc010ff5435aa183ec6d3f02/megatron/optimizer/optimizer.py#L618">in Megatron-DS</a>, and in contemporary DeepSpeed’s <a href="https://github.com/microsoft/DeepSpeed/blob/1d1a20c5a1dd980438393500f14d8dc188f6258a/deepspeed/runtime/fp16/unfused_optimizer.py#L224">unfused optimiser</a>.</details>
</p>

The above mixed-precision costs assume that other features such as gradient accumulation and distributed training are **disabled**. We will now consider the costs of those features _in isolation_.

### Estimated cost of gradient accumulation

Gradient accumulation is a technique which computes the gradients for a minibatch, one microbatch at a time, summing each result (divided by the number of microbatches). This is typically done to save memory (at the expense of wall time), for a given minibatch size.  
In distributed training, gradient accumulation can also be exploited to reduce the frequency with which synchronization of gradients is required, reducing the proportion of training time spent on communication overhead.

When running in mixed-precision: we expect gradient accumulation to cost:

<table class="lm-wireframe lm-lefthead lm-greyhead lm-prose-dense">
  <thead>
    <tr>
      <th><abbr title="Megatron-DeepSpeed">Megatron-DS</abbr>, <abbr title="DeepSpeed">DS</abbr> ZeRO</th>
      <th>PyTorch <abbr title="Automated Mixed Precision">AMP</abbr></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0 or 4 bytes/param*</td>
      <td>0 bytes/param**</td>
    </tr>
  </tbody>
</table>

<p>
<details><summary>*Deepspeed offers two gradient accumulation modes, based on whether you accumulate and reduce gradients using the same datatype. If the datatypes match: you can use the same buffer for both, saving memory.</summary>
This is an unrealistic/legacy option though; it is better to reduce in half-precision (to send less data over the network in distributed training) and accumulate in float32 (accumulators benefit from increased precision). This requires the allocation of an additional float32 buffer, costing 4 bytes/param.</details>
<details><summary>**In PyTorch <abbr title="Automated Mixed Precision">AMP</abbr>: we already paid for a buffer that can be used for gradient accumulation.</summary>
Our backwards pass and optimisation step are not fused together; we store float32 gradients, then we step our optimiser. The backward pass accumulates the half-precision compute gradients into this float32 gradient, whether we do 1 microstep or many.</details>
</p>

### Estimated cost of distributed training

There are <a href="https://github.com/stas00/ml-engineering/tree/master/model-parallelism">many ways</a> to distribute training. Among the simplest is [DDP](https://arxiv.org/abs/2006.15704) (Distributed Data Parallel), which replicates the model across multiple GPUs. We wish to measure its costs here, because its ease-of-use makes it a good fit for hobbyist or small-scale training.

<abbr title="Distributed Data Parallel">DDP</abbr> doesn’t reduce the size of the model or optimiser states. It enables us to achieve bigger minibatch sizes, or give smaller microbatches to each GPU, reducing the per-GPU cost of activations. The experiment we’re doing here has already minimized the cost of activations. Hence we don’t expect <abbr title="Distributed Data Parallel">DDP</abbr> to save us memory, but we are interested in measuring the overhead incurred by the extra communication it necessitates.

We expect PyTorch <abbr title="Distributed Data Parallel">DDP</abbr> to cost <strong>4 bytes/param</strong>, as its distributed Reducer <a href="https://discuss.pytorch.org/t/memory-consumption-for-the-model-get-doubled-after-wrapped-with-ddp/130837/5">creates gradient buckets for each parameter</a>. This cost can be reduced by enabling <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"><code>gradient_as_bucket_view</code></a>.

The frameworks studied in Transformer Math can be distributed in more exotic ways than <abbr title="Distributed Data Parallel">DDP</abbr>, capable of sharding the model and optimiser states. DeepSpeed ZeRO allocates a ~477MiB bucket for <a href="https://github.com/microsoft/DeepSpeed/blob/a3926bbbf6d0025b5c6076a280e6b91ebd08aada/deepspeed/runtime/zero/stage_1_and_2.py#L663">allreduce operations</a>, a fixed cost that should not scale with model size.

## What we observed experimentally

We trained with batch-of-1, short sequences, and a stateless optimiser for 2 steps.  
Where gradient accumulation was used: we ran each step for 3 microsteps.

For 7b models, we used <a href="https://huggingface.co/docs/transformers/v4.17.0/en/parallelism#naive-model-parallelism-vertical-and-pipeline-parallelism">naïve model parallelism</a> to vertically-shard costs over two 48GiB GPUs. This type of parallelism is non-performant and therefore unrealistic, but serves as a good way to simulate having one larger GPU.

<em>Note also that when Nvidia markets the capacity of a GPU, they use the measure '<a href="https://simple.wikipedia.org/wiki/Gigabyte">GB</a>' but they actually mean <a href="https://simple.wikipedia.org/wiki/Gibibyte">GiB</a>.</em>

We were not able to measure <abbr title="Distributed Data Parallel">DDP</abbr> costs 7b models, as we had no remaining GPUs over which to replicate the training.

### Memory cost (MiB)

<table class="lm-wireframe lm-lefthead lm-greyhead lm-dense">
  <thead>
    <tr>
      <th rowspan="4" class="bottom center">Arch</th>
      <th rowspan="4" class="bottom center">Model</th>
      <th rowspan="4" class="bottom center">Param (b)</th>
      <th class="right"><abbr title="Distributed Data Parallel">DDP</abbr></th>
      <th colspan="4" class="off">Off</th>
      <th colspan="4" class="on">On</th>
    </tr>
    <tr>
      <th class="right"><abbr title="Automated Mixed Precision">AMP</abbr></th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
    </tr>
    <tr>
      <th class="right"><abbr title="Gradient Accumulation">GA</abbr></th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
    </tr>
    <tr>
      <th/>
      <th colspan="8" class="center">Total memory used + reserved (GiB)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3">GPT-NeoX</th>
      <th>pythia-1.4b</th>
      <th title="1,414,647,808">1.4</th>
      <th rowspan="5"/>
      <td class="datum">10.7</td>
      <td class="datum">11.1</td>
      <td class="datum">12.7</td>
      <td class="datum">13.8</td>
      <td class="datum">16.1</td>
      <td class="datum">16.6</td>
      <td class="datum">16.3</td>
      <td class="datum">19.5</td>
    </tr>
    <tr>
      <th>pythia-2.8b</th>
      <th title="2,775,208,960">2.8</th>
      <td class="datum">21.0</td>
      <td class="datum">21.5</td>
      <td class="datum">25.0</td>
      <td class="datum">27.4</td>
      <td class="datum">31.8</td>
      <td class="datum">32.1</td>
      <td class="datum">32.4</td>
      <td class="datum">37.6</td>
    </tr>
    <tr>
      <th>pythia-6.9b</th>
      <th title="6,857,302,016">6.9</th>
      <td class="datum">51.4</td>
      <td class="datum">53.1</td>
      <td class="datum">61.7</td>
      <td class="datum">67.8</td>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
    <tr>
      <th rowspan="3">Llama</th>
      <th>openllama-3b</th>
      <th title="3,426,473,600">3.4</th>
      <td class="datum">25.9</td>
      <td class="datum">26.3</td>
      <td class="datum">29.8</td>
      <td class="datum">34.6</td>
      <td class="datum">39.5</td>
      <td class="datum">39.5</td>
      <td class="datum">40.2</td>
      <td class="oom">&gt;48</td>
    </tr>
    <tr>
      <th>llama-2-7b</th>
      <th title="6,738,415,616">6.7</th>
      <td class="datum">50.5</td>
      <td class="datum">51.6</td>
      <td class="datum">57.4</td>
      <td class="datum">67.0</td>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
  </tbody>
</table>

We can see that almost none of these training configurations fit on an Nvidia RTX 4090 (24GiB), the largest consumer Nvidia GPU at the time of writing.  
It’s even a tight fit on a lot of server Nvidia GPUs (e.g. 40GiB and 48GiB). Bear in mind how these training configurations constitute a _lower bound_ (we aren’t paying for optimiser state, and our activations are unrealistically small).

We see that enabling _all_ of these training techniques is punitive, doubling our memory costs.

It’s clear why Nvidia’s top-end 80GiB GPUs are desirable, and why there is interest in <a href="https://www.mosaicml.com/blog/amd-mi250">training on AMD’s 128GiB GPUs</a>. We also see that realistically, the training of large models must rely upon the sharding of memory costs across GPUs.

Let’s now try to understand these costs as a function of model size.

### Memory cost (bytes/param)

Here we divide the memory usage by the number of model parameters.

<table class="lm-wireframe lm-lefthead lm-greyhead lm-dense">
  <thead>
    <tr>
      <th rowspan="4" class="bottom center">Arch</th>
      <th rowspan="4" class="bottom center">Model</th>
      <th rowspan="4" class="bottom center">Param (b)</th>
      <th class="right"><abbr title="Distributed Data Parallel">DDP</abbr></th>
      <th colspan="4" class="off">Off</th>
      <th colspan="4" class="on">On</th>
    </tr>
    <tr>
      <th class="right"><abbr title="Automated Mixed Precision">AMP</abbr></th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
    </tr>
    <tr>
      <th class="right"><abbr title="Gradient Accumulation">GA</abbr></th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
    </tr>
    <tr>
      <th/>
      <th colspan="8" class="center">bytes/param</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3">GPT-NeoX</th>
      <th>pythia-1.4b</th>
      <th>1.4</th>
      <th rowspan="5"/>
      <td class="datum">8.1</td>
      <td class="datum">8.4</td>
      <td class="datum">9.6</td>
      <td class="datum">10.5</td>
      <td class="datum">12.2</td>
      <td class="datum">12.6</td>
      <td class="datum">12.3</td>
      <td class="datum">14.8</td>
    </tr>
    <tr>
      <th>pythia-2.8b</th>
      <th>2.8</th>
      <td class="datum">8.1</td>
      <td class="datum">8.3</td>
      <td class="datum">9.7</td>
      <td class="datum">10.6</td>
      <td class="datum">12.3</td>
      <td class="datum">12.4</td>
      <td class="datum">12.5</td>
      <td class="datum">14.5</td>
    </tr>
    <tr>
      <th>pythia-6.9b</th>
      <th>6.9</th>
      <td class="datum">8.0</td>
      <td class="datum">8.3</td>
      <td class="datum">9.7</td>
      <td class="datum">10.6</td>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
    <tr>
      <th rowspan="3">Llama</th>
      <th>openllama-3b</th>
      <th>3.4</th>
      <td class="datum">8.1</td>
      <td class="datum">8.2</td>
      <td class="datum">9.3</td>
      <td class="datum">10.9</td>
      <td class="datum">12.4</td>
      <td class="datum">12.4</td>
      <td class="datum">12.6</td>
      <td class="oom">&gt;15.0</td>
    </tr>
    <tr>
      <th>llama-2-7b</th>
      <th>6.7</th>
      <td class="datum">8.0</td>
      <td class="datum">8.2</td>
      <td class="datum">9.1</td>
      <td class="datum">10.7</td>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
  </tbody>
</table>

Remarkably, some very regular patterns emerge, scaling moreorless linearly with model size.

Our (strawman) baseline of "everything in float32" is indeed 8 bytes/param. This baseline should be low-overhead; it will incur less fragmentation, as it does not need to allocate as many temporary buffers as the other configurations.

Our estimate that "mixed-precision should cost 10 bytes/param" looks close (it’s actually slightly cheaper), but the theory "<abbr title="Distributed Data Parallel">DDP</abbr> costs 4 bytes/param more" only holds true when gradient accumulation is enabled. Something to dig into.

Let’s visualise these datapoints as relative overheads compared to our float32 baseline.

### Memory cost (bytes/param), relative to float32 baseline

<table class="lm-wireframe lm-lefthead lm-greyhead lm-dense">
  <thead>
    <tr>
      <th rowspan="4" class="bottom center">Arch</th>
      <th rowspan="4" class="bottom center">Model</th>
      <th rowspan="4" class="bottom center">Param (b)</th>
      <th class="right"><abbr title="Distributed Data Parallel">DDP</abbr></th>
      <th colspan="4" class="off">Off</th>
      <th colspan="4" class="on">On</th>
    </tr>
    <tr>
      <th class="right"><abbr title="Automated Mixed Precision">AMP</abbr></th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
    </tr>
    <tr>
      <th class="right"><abbr title="Gradient Accumulation">GA</abbr></th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
    </tr>
    <tr>
      <th/>
      <th colspan="8" class="center">Relative overhead (bytes/param)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3">GPT-NeoX</th>
      <th>pythia-1.4b</th>
      <th>1.4</th>
      <th rowspan="5"/>
      <td class="datum">0.00</td>
      <td class="datum">0.29</td>
      <td class="datum">1.52</td>
      <td class="datum">2.34</td>
      <td class="datum">4.08</td>
      <td class="datum">4.45</td>
      <td class="datum">4.22</td>
      <td class="datum">6.65</td>
    </tr>
    <tr>
      <th>pythia-2.8b</th>
      <th>2.8</th>
      <td class="datum">0.00</td>
      <td class="datum">0.18</td>
      <td class="datum">1.56</td>
      <td class="datum">2.45</td>
      <td class="datum">4.19</td>
      <td class="datum">4.27</td>
      <td class="datum">4.39</td>
      <td class="datum">6.41</td>
    </tr>
    <tr>
      <th>pythia-6.9b</th>
      <th>6.9</th>
      <td class="datum">0.00</td>
      <td class="datum">0.28</td>
      <td class="datum">1.62</td>
      <td class="datum">2.57</td>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
    <tr>
      <th rowspan="3">Llama</th>
      <th>openllama-3b</th>
      <th>3.4</th>
      <td class="datum">0.00</td>
      <td class="datum">0.12</td>
      <td class="datum">1.20</td>
      <td class="datum">2.73</td>
      <td class="datum">4.25</td>
      <td class="datum">4.26</td>
      <td class="datum">4.47</td>
      <td class="oom">&gt;6.91</td>
    </tr>
    <tr>
      <th>llama-2-7b</th>
      <th>6.7</th>
      <td class="datum">0.00</td>
      <td class="datum">0.18</td>
      <td class="datum">1.10</td>
      <td class="datum">2.63</td>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
  </tbody>
</table>

Some results here match our expectations, but there are some surprises also.

Gradient accumulation on its own is almost free (as expected), but it has interesting interactions depending on which other features are enabled. Using it in concert with mixed-precision costs more than the sum of either feature’s cost in isolation.

<p>
<details><summary>Mixed-precision on its own is cheaper than the 2 bytes/param that we predicted. Perhaps PyTorch <abbr title="Automated Mixed Precision">AMP</abbr> found situations where it could reclaim freed temporary buffers.</summary>
This could be possible if (for example) it opted to recreate half-precision compute weights rather than keep them around.
</details>
</p>
<p>
<details><summary>We <em>do</em> expect mixed-precision to cost slightly below 2 bytes/param due to embeddings and norms’ not requiring half-precision copies (they are computed in float32). However, the discrepancy we’re seeing seems too large to be closed by that theory.</summary>
Discounting a half-precision copy of pythia 6.9b’s embedding layer would only explain 0.06 bytes/param of this undershoot (and another 0.00 bytes/param if we consider norm params).</details>
</p>

<abbr title="Distributed Data Parallel">DDP</abbr> in isolation costs about 4 bytes/param, as expected. This can be reduced by enabling <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"><code>gradient_as_bucket_view</code></a>. We conducted a limited exploration of its benefits:

<table class="lm-wireframe lm-lefthead lm-greyhead lm-dense">
  <thead>
    <tr>
      <th rowspan="5" class="bottom center">Arch</th>
      <th rowspan="5" class="bottom center">Model</th>
      <th class="right"><abbr title="Distributed Data Parallel">DDP</abbr></th>
      <th colspan="4" class="on">On</th>
      <th rowspan="4" colspan="2" class="bottom center">Memory saved</th>
    </tr>
    <tr>
      <th class="right">Mixed Prec</th>
      <th colspan="4" class="on">On</th>
    </tr>
    <tr>
      <th class="right">Grad Acc</th>
      <th colspan="4" class="on">On</th>
    </tr>
    <tr>
      <th class="right">Bucket view</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
    </tr>
    <tr>
      <th/>
      <th colspan="2" class="center">Mem (GiB)</th>
      <th colspan="2" class="center">bytes/param</th>
      <th class="center">GiB</th>
      <th class="center">b/param</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2">GPT-NeoX</th>
      <th>pythia-1.4b</th>
      <th rowspan="2"/>
      <td class="datum">19.5</td>
      <td class="datum">16.1</td>
      <td class="datum">14.77</td>
      <td class="datum">12.20</td>
      <td class="datum">3.38</td>
      <td class="datum">2.57</td>
    </tr>
    <tr>
      <th>pythia-2.8b</th>
      <td class="datum">37.8</td>
      <td class="datum">31.4</td>
      <td class="datum">14.61</td>
      <td class="datum">12.16</td>
      <td class="datum">6.33</td>
      <td class="datum">2.45</td>
    </tr>
  </tbody>
</table>

The pattern appears to scale with model size; enabling <code>gradient_as_bucket_view</code> seems to save about 2.5 bytes/param on the cost of <abbr title="Distributed Data Parallel">DDP</abbr>.

<abbr title="Distributed Data Parallel">DDP</abbr>, <abbr title="Automated Mixed Precision">AMP</abbr> and <abbr title="Gradient Accumulation">GA</abbr> exhibit interesting interactions with each other. Let’s determine "how much would it cost to enable X, given Y and/or Z are enabled".

### Memory cost (bytes/param) of enabling mixed-precision

Given a starting point of "<abbr title="Distributed Data Parallel">DDP</abbr> and <abbr title="Gradient Accumulation">GA</abbr> are enabled/disabled": how much does it cost to enable mixed-precision?

<table class="lm-wireframe lm-lefthead lm-greyhead lm-dense">
  <thead>
    <tr>
      <th rowspan="3" class="bottom center">Arch</th>
      <th rowspan="3" class="bottom center">Model</th>
      <th class="right"><abbr title="Distributed Data Parallel">DDP</abbr></th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
    </tr>
    <tr>
      <th class="right"><abbr title="Gradient Accumulation">GA</abbr></th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
    </tr>
    <tr>
      <th/>
      <th colspan="4" class="center"><abbr title="Automated Mixed Precision">AMP</abbr> cost (bytes/param)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3">GPT-NeoX</th>
      <th>pythia-1.4b</th>
      <th rowspan="5"/>
      <td class="datum">1.52</td>
      <td class="datum">2.05</td>
      <td class="datum">0.14</td>
      <td class="datum">2.20</td>
    </tr>
    <tr>
      <th>pythia-2.8b</th>
      <td class="datum">1.56</td>
      <td class="datum">2.27</td>
      <td class="datum">0.21</td>
      <td class="datum">2.13</td>
    </tr>
    <tr>
      <th>pythia-6.9b</th>
      <td class="datum">1.62</td>
      <td class="datum">2.29</td>
      <td class="null"/>
      <td class="null"/>
    </tr>
    <tr>
      <th rowspan="3">Llama</th>
      <th>openllama-3b</th>
      <td class="datum">1.20</td>
      <td class="datum">2.61</td>
      <td class="datum">0.22</td>
      <td class="oom">OOM</td>
    </tr>
    <tr>
      <th>llama-2-7b</th>
      <td class="datum">1.10</td>
      <td class="datum">2.44</td>
      <td class="null"/>
      <td class="null"/>
    </tr>
  </tbody>
</table>

<p>
<details><summary>If you’re already paying for <abbr title="Distributed Data Parallel">DDP</abbr>: mixed-precision can be added essentially for free.</summary>
This indicates that some memory re-use becomes possible. Perhaps because mixed-precision and <abbr title="Distributed Data Parallel">DDP</abbr> require memory at different times: half-precision compute copies can reside in the <abbr title="Distributed Data Parallel">DDP</abbr> allreduce buffer, which is needed only <em>after</em> the mixed-precision results have already been computed.</details>
</p>

<p>
<details>
<summary>When gradient accumulation is off: mixed-precision costs <em>less</em> than the 2 bytes/param upper bound we predicted.</summary>
<p>
This could be because a layer’s float32 gradients and its half-precision compute copy don't need to exist at the same time. At least, if we meet two conditions:
</p>
<ul>
  <li>Conclude training steps by deleting the model’s gradients (<code>Optimizer.zero_grad(set_to_none=True)</code>)</li>
  <li>Disable gradient accumulation</li>
</ul>
<p>
These conditions are satisfied by our experimental setup. Consequently, there is an opportunity for memory re-use.
</p>
<p>
Perhaps when the backward pass frees <em>two</em> layers’ half-precision compute copies: the memory is used to pay for <em>one</em> layer’s float32 gradients. This would halve the 4 bytes/param cost we reserved for float32 gradients (as only half of our layers would need to allocate new memory). As compelling as this theory is: we would expect a <em>0</em>bytes/param overhead for enabling mixed-precision if this were the case (as mixed-precision would subsidise a cost we were already paying for). Moreover it makes assumptions that two pages could be merged into a bigger contiguous page; we are not convinced this happens.
</p>

<p>
An easier explanation could be that the lifetime of a compute copy is limited, and some layers recreate their compute copies from the float32 model parameters.
</p>
</details>
</p>

<p>
<details><summary>When gradient accumulation is <em>on</em>: enabling mixed-precision costs <em>more</em> than the 2 bytes/param that we thought it could cost in theory.</summary>
This could indicate that less memory re-use is possible, or that more fragmentation occurs.</details>
</p>

### Memory cost (bytes/param) of enabling Gradient accumulation

Given a starting point of "<abbr title="Distributed Data Parallel">DDP</abbr> and <abbr title="Automated Mixed Precision">AMP</abbr> are enabled/disabled": how much does it cost to enable gradient accumulation?

<table class="lm-wireframe lm-lefthead lm-greyhead lm-dense">
  <thead>
    <tr>
      <th rowspan="3" class="bottom center">Arch</th>
      <th rowspan="3" class="bottom center">Model</th>
      <th class="right"><abbr title="Distributed Data Parallel">DDP</abbr></th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
    </tr>
    <tr>
      <th class="right"><abbr title="Automated Mixed Precision">AMP</abbr></th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
    </tr>
    <tr>
      <th/>
      <th colspan="4" class="center"><abbr title="Gradient Accumulation">GA</abbr> cost (bytes/param)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3">GPT-NeoX</th>
      <th>pythia-1.4b</th>
      <th rowspan="5"/>
      <td class="datum">0.29</td>
      <td class="datum">0.82</td>
      <td class="datum">0.37</td>
      <td class="datum">2.43</td>
    </tr>
    <tr>
      <th>pythia-2.8b</th>
      <td class="datum">0.18</td>
      <td class="datum">0.89</td>
      <td class="datum">0.09</td>
      <td class="datum">2.01</td>
    </tr>
    <tr>
      <th>pythia-6.9b</th>
      <td class="datum">0.28</td>
      <td class="datum">0.95</td>
      <td class="null"/>
      <td class="null"/>
    </tr>
    <tr>
      <th rowspan="3">Llama</th>
      <th>openllama-3b</th>
      <td class="datum">0.12</td>
      <td class="datum">1.53</td>
      <td class="datum">0.01</td>
      <td class="oom">OOM</td>
    </tr>
    <tr>
      <th>llama-2-7b</th>
      <td class="datum">0.18</td>
      <td class="datum">1.53</td>
      <td class="null"/>
      <td class="null"/>
    </tr>
  </tbody>
</table>

Both <abbr title="Distributed Data Parallel">DDP</abbr> and <abbr title="Automated Mixed Precision">AMP</abbr> have adverse interactions with gradient accumulation. Enabling either will increase the cost, but the cost of enabling both is greater than the sum of their individual overheads.

This is an overhead we <a href="https://github.com/huggingface/accelerate/issues/2035">encountered previously</a> when using <abbr title="HuggingFace">HF</abbr> accelerate libraries, but now we see that it reproduces in pure PyTorch.

### Memory cost (bytes/param) of enabling <abbr title="Distributed Data Parallel">DDP</abbr>

Given a starting point of "<abbr title="Automated Mixed Precision">AMP</abbr> and <abbr title="Gradient Accumulation">GA</abbr> are enabled/disabled": how much does it cost to enable <abbr title="Distributed Data Parallel">DDP</abbr>?

<table class="lm-wireframe lm-lefthead lm-greyhead lm-dense">
  <thead>
    <tr>
      <th rowspan="3" class="bottom center">Arch</th>
      <th rowspan="3" class="bottom center">Model</th>
      <th class="right"><abbr title="Automated Mixed Precision">AMP</abbr></th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
    </tr>
    <tr>
      <th class="right"><abbr title="Gradient Accumulation">GA</abbr></th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
    </tr>
    <tr>
      <th/>
      <th colspan="4" class="center"><abbr title="Distributed Data Parallel">DDP</abbr> cost (bytes/param)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3">GPT-NeoX</th>
      <th>pythia-1.4b</th>
      <th rowspan="5"/>
      <td class="datum">4.08</td>
      <td class="datum">4.16</td>
      <td class="datum">2.70</td>
      <td class="datum">4.31</td>
    </tr>
    <tr>
      <th>pythia-2.8b</th>
      <td class="datum">4.19</td>
      <td class="datum">4.09</td>
      <td class="datum">2.83</td>
      <td class="datum">3.95</td>
    </tr>
    <tr>
      <th>pythia-6.9b</th>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
    <tr>
      <th rowspan="3">Llama</th>
      <th>openllama-3b</th>
      <td class="datum">4.25</td>
      <td class="datum">4.14</td>
      <td class="datum">3.27</td>
      <td class="oom">>4.18</td>
    </tr>
    <tr>
      <th>llama-2-7b</th>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
  </tbody>
</table>

In most cases, it costs 4 bytes/param to enable <abbr title="Distributed Data Parallel">DDP</abbr>. There is one synergetic case, whereby if mixed-precision is used _without_ gradient accumulation, then the cost of <abbr title="Distributed Data Parallel">DDP</abbr> decreases.

## Conclusions

The memory costs we derived for PyTorch <abbr title="Automated Mixed Precision">AMP</abbr> aren’t a million miles away from what’s described in Transformer Math. Moreover, these heuristics predicted our empirical results reasonably well.

The training techniques studied here incurred greater overheads when used in combination than when used individually, perhaps due to memory fragmentation. Some combinations appeared to exhibit synergies, perhaps due to re-use of freed temporary memory.

For <abbr title="Distributed Data Parallel">DDP</abbr> specifically, we learned that <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"><code>gradient_as_bucket_view</code></a> can be enabled, to save a huge amount of memory (~2.5 bytes/param).

With all techniques enabled (mixed-precision training, gradient accumulation, <abbr title="Distributed Data Parallel">DDP</abbr>): our lower bound is somewhere in the region of 15 bytes/param.

<p>
<details><summary>
Add a realistic optimiser (32-bit <a href="https://arxiv.org/abs/1412.6980">Adam</a><a href="https://arxiv.org/abs/1711.05101">W</a>*) and that increases to 23 bytes/param, or 145GiB for llama 7b. This exceeds the capacity of most GPUs on the market.</summary>
It could fit on an <a href="https://www.tomshardware.com/news/amd-expands-mi300-with-gpu-only-model-eight-gpu-platform-with-15tb-of-hbm3">AMD MI300X 192GB</a>!<br> 
<em>*More exotic optimisers exist, with lower memory requirements, such as <a href="https://arxiv.org/abs/2110.02861">8-bit AdamW</a>. 32-bit AdamW is a good place to start if you have enough memory.</em>
</details>
</p>

It’s clear that beginner-level techniques won’t cut it for fine-tuning LLMs; we need to shard the model parameters, gradients, optimiser states and data over multiple GPUs, trading <abbr title="Distributed Data Parallel">DDP</abbr> for <a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">more advanced techniques</a> such as <a href="https://arxiv.org/abs/1811.06965">PP</a> (pipeline parallelism), <a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/">FSDP</a> (fully-sharded data parallel), <a href="https://arxiv.org/abs/1910.02054">ZeRO</a> (Zero Redundancy Optimizer), and/or <a href="https://huggingface.co/docs/text-generation-inference/conceptual/tensor_parallelism">TP</a> (tensor parallelism).

<p>
<details><summary>
Realistic activations for a model such as llama-2 7b could be 1GiB per batch item** (0.16 bytes/param), and a realistic batch size <a href="https://github.com/tatsu-lab/stanford_alpaca#fine-tuning">could be 128</a> or higher. That’s another 128GiB we will only fit by distributing over many GPUs, or by serializing into microbatches via gradient accumulation.</summary>
<em>**Transformer Math “full recomputation” formula (so this is a lower bound, using gradient checkpointing), for sequence length 4096.</em>
</details>
</p>

As daunting as this looks: we should spare a thought for those who confront _pretraining_ (training a model from scratch), which entails larger batch sizes, longer training runs, and more failed attempts. It can cost <a href="https://www.mosaicml.com/blog/mpt-7b">$250k to pretrain a 7b model</a>, and 7b is on the small side for LLMs!

But there is an easier way! For the rest of us, <a href="https://github.com/huggingface/peft">parameter-efficient finetuning</a> methods exist, which drastically lower the compute requirements, making fine-tuning possible on consumer GPUs. We hope to explore these techniques in a future blog post.

## Citing

<pre>
@article{birch2023llmmem,
  title   = "LLM finetuning: memory requirements, expectation vs reality",
  author  = "Birch, Alex",
  journal = "blog.scottlogic.com",
  year    = "2023",
  url     = "{{ page.url | absolute_url }}"
}
</pre>

## References

_Articles and papers for which we found a BibTeX citation:_

<!--
{
const citeSet = new Set();
const cites = [];
for (const anchor of document.querySelectorAll('a[href]')) {
  const { href } = anchor;
  const { host } = new URL(href);
  if (!citeSet.has(href) && host.includes('arxiv')) {
    const { innerText } = anchor;
    citeSet.add(href);
    cites.push({ href, innerText });
  }
}
cites.map(({href, innerText}) => `${href} ${innerText}`).join('\n')
}
-->

[1] Anthony et al. [Transformer Math 101](https://blog.eleuther.ai/transformer-math/) [blog.eleuther.ai](https://blog.eleuther.ai/) 2023  
[2] Li et al. [PyTorch Distributed: Experiences on Accelerating Data Parallel Training](https://arxiv.org/abs/2006.15704) VLDB 2020  
[3] Dao et al. [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135) arXiv preprint   arXiv:2205.14135 (2022)  
[4] Kingma et al. [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) ICLR 2015  
[5] Loshchilov et al. [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) ICLR 2019  
[5] Dettmers et al. [8-bit Optimizers via Block-wise Quantization](https://arxiv.org/abs/2110.02861) ICLR 2022  
[6] Huang et al. [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/abs/1811.06965) CVPR 2018  
[7] Rajbhandari et al. [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054) arXiv preprint arXiv:1910.02054 (2019)

<!--
https://arxiv.org/abs/2006.15704 DDP
https://arxiv.org/abs/2205.14135 Flash Attention
https://arxiv.org/abs/1412.6980 Adam
https://arxiv.org/abs/1711.05101 W
https://arxiv.org/abs/2110.02861 8-bit AdamW
https://arxiv.org/abs/1811.06965 PP
https://arxiv.org/abs/1910.02054 ZeRO
-->
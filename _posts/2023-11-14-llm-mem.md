---
title: 'LLM finetuning: memory requirements, expectation vs reality'
date: 2023-11-14 00:00:00 Z
categories:
- abirch
- Tech
author: abirch
layout: default_post
summary: The memory costs for LLM training are large but predictable.
---

<style>
  table.lm-lefthead tbody th {
    text-align: left;
  }
  table.lm-greyhead tbody th {
    background-color: #f8f8f8;
  }
  table.lm-wireframe tbody tr {
    background-color: initial;
  }
  table.lm-wireframe th, table.lm-wireframe td {
    border: 1px solid;
  }
  details {
    font-weight: 300;
  }
  summary {
    font-weight: 300;
    display: block;
  }
  summary::after {
    cursor: pointer;
    content: '[+more]';
    text-decoration: underline;
    text-decoration-style: dotted;
    padding-left: 0.5em;
    font-size: 0.8em;
  }
  details[open] > summary::after {
    content: ' [−less]';
  }
  .default-post .post-content a.text-ref {
    text-decoration: underline;
    text-decoration-style: dotted;
    text-decoration-thickness: 1px;
    text-decoration-color: black;
    border-bottom: initial;
  }
  .reset-fontweight {
    font-weight: 300;
  }
  .right {
    text-align: right;
  }
  .center {
    text-align: center;
  }
  .off {
    background-color: #ffcccc;
    text-align: center;
  }
  .on {
    background-color: #d9ead3;
    text-align: center;
  }
  .datum {
    font-family: monospace;
    text-align: right;
  }
  .null {
    background-color: #f3f3f3;
  }
  .oom {
    font-family: monospace;
    text-align: right;
    background-color: #dd7e6b;
  }
  .bottom {
    vertical-align: bottom;
  }
</style>

If you want to try your hand at fine-tuning an LLM (Large Language Model): one of the first things you’re going to need to know is "will it fit on my GPU".

The pre-eminent guide to estimating (VRAM) memory requirements is [Transformer Math 101](https://blog.eleuther.ai/transformer-math/). It bears mentioning, though, that its heuristics are written in the context of frameworks such as GPT-NeoX and Megatron-DeepSpeed. These are specialist frameworks for training at scale, and are not what a beginner would reach for.

Are Transformer Math’s memory estimates applicable to idiomatic PyTorch, without external frameworks? Let’s put it to the test.

## Experimental setup

We built a simple training loop ([`mem_llm.py`](https://github.com/scottlogic-alex/qlora/blob/memory-investigation/mem_llm.py)). It loads a model via HF (HuggingFace) transformers, and trains said model to complete random sequences.
By running this training loop for a few steps, then taking a measurement: we can determine our peak memory usage.

We will evaluate the cost of three common techniques provided by an ML accelerator (explanations to follow later):

- [Mixed-precision](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) computation
- [DDP](https://arxiv.org/abs/2006.15704) (Distributed Data Parallel) parallelism
- [Gradient accumulation](https://medium.com/mlearning-ai/gradient-accumulation-307de7599e87)

 <!-- The <abbr title="World Health Organization">WHO</abbr> was founded in 1948.  -->

In our trainer: this functionality is built using PyTorch idioms. Our implementation is similar to how <abbr title="HuggingFace">HF</abbr> Accelerate operates in its most basic configurations (i.e. when it is not configured to delegate to other accelerators such as DeepSpeed or Megatron).

## What we expect to see

Before we dive into measurements: let’s estimate how much memory we’ll need in theory. We will use reasoning similar to Transformer Math.

For _non-distributed_ training,  
```
Total Training Memory = Model + Optimiser + Activations + Gradients
```

<p id="no-optimiser-cost">
Our experimental setup <strong>eliminates optimiser costs</strong>, through means of a <em>stateless optimizer</em> (SGD without momentum).
</p>

<p id="low-activation-cost">
<details><summary>We also <strong>minimise activations</strong>, by using <em>tiny batch sizes</em> and <em>sequence lengths</em>.</summary>At batch-of-1, sequence length 8: we expect activations to be negligible (<100MiB) even on our largest model (Llama 7b), according to Transformer Math's worst-case activations formula, "no recomputation" (i.e. gradient checkpointing disabled).</details>
</p>

These measures narrow down the possible sources of overhead, if detected.

### Estimated baseline costs in single-precision

First up, let's establish the terminology we're going to use:

<table class="lm-wireframe lm-lefthead lm-greyhead">
  <thead>
    <th>Precision</th>
    <th>Datatype(s)</th>
    <th>Bytes per element</th>
  </thead>
  <tbody>
    <tr>
      <td>Single</td>
      <td>
        <code class="highlighter-rouge">float32</code>
      </td>
      <td>4</td>
    </tr>
    <tr>
      <td>Half</td>
      <td>
        <code class="highlighter-rouge">float16</code>,
        <code class="highlighter-rouge">bfloat16</code>
      </td>
      <td>2</td>
    </tr>
  </tbody>
</table>

The memory costs for pure-float32 training are relatively straightforward:

<table class="lm-wireframe lm-lefthead lm-greyhead">
  <tbody>
    <tr>
      <th rowspan="2">Model</th>
      <th>Params</th>
      <td>4 bytes/param</td>
    </tr>
    <tr>
      <th>Gradients</th>
      <td>4 bytes/param</td>
    </tr>
    <tr>
      <th colspan="2">Optimizer states</th>
      <td><a class="text-ref" href="#no-optimiser-cost">None</a> in our setup*</td>
    </tr>
    <tr>
      <th colspan="2">Activations</th>
      <td><a class="text-ref" href="#low-activation-cost">Negligible</a> in our setup**</td>
    </tr>
  </tbody>
  <tfoot>
    <tr>
      <th colspan="2">Total</th>
      <td>8 bytes/param</td>
    </tr>
  </tfoot>
</table>

<p>
<details><summary>* It's worth noting that our use of a stateless optimiser is highly unrealistic; typical optimiser costs are <em>substantial</em>.</summary>
Enlisting an uncontroversial optimiser such as 32-bit AdamW would cost an additional 8 bytes/param.</details>
<details><summary>** Likewise, activations would <em>normally</em> be a substantial cost, easily capable of growing bigger than all other costs combined.</summary>
Even more so in pure-float32 training, where our activations will be 32-bit. By comparison: mixed-precision training enables activations to be smaller (employing the smaller datatype used for reduced-precision compute).</details>
</p>

We don’t dwell on this baseline, because it is a bit of a strawman. Floating-point operations are slow in 32-bit; they don’t utilize tensor cores, and lack accelerated kernels for essential operations such as Flash Attention.

For these reasons, and to reduce the cost of activations: it is standard to train in mixed-precision.

### Estimated baseline costs in mixed-precision

Mixed-precision costs are framework-specific. We will interpret the costs for the frameworks considered in Transformer Math, and also make an educated guess at the costs of PyTorch’s built-in mixed-precision implementation, AMP.

Transformer Math was validated against GPT-NeoX (circa April 2023), and was cross-checked against Megatron-DeepSpeed (against which its predictions held, for everything except activation checkpointing). Its estimates are expected to be predictive to within ±10% including on other accelerators such as Megatron-LM, PyTorch FSDP, and ColossalAI.  
_Thanks go to Quentin Anthony at EleutherAI for confirming the above._

GPT-NeoX implements mixed-precision via DeepSpeed ZeRO. In both DeepSpeed ZeRO and Megatron-DS: the model is held in half-precision, whilst the optimizer takes a float32 copy of the parameters.

PyTorch AMP (Automated Mixed Precision) implements mixed-precision moreorless the opposite way around. The model is held in float32, and AMP manages a half-precision copy.

My understanding is as follows:

<table class="lm-wireframe lm-lefthead lm-greyhead">
  <thead>
    <tr>
      <th colspan="2"/>
      <th>Megatron-DS, Deepspeed</th>
      <th>Pytorch AMP</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2">Model</th>
      <th>Params</th>
      <td>2 bytes/param</td>
      <td>4 bytes/param</td>
    </tr>
    <tr>
      <th>Gradients</th>
      <td>2 bytes/param</td>
      <td>4 bytes/param</td>
    </tr>
    <tr>
      <th rowspan="2">Compute Copy</th>
      <th>Params</th>
      <td rowspan="2">None</td>
      <td>2 bytes/param*</td>
    </tr>
    <tr>
      <th>Gradients</th>
      <td>0 bytes/param**</td>
    </tr>
    <tr>
      <th rowspan="2">Optimizer Copy</th>
      <th>Params</th>
      <td>4 bytes/param</td>
      <td rowspan="2">None</td>
    </tr>
    <tr>
      <th>Gradients</th>
      <td>4 bytes/param*** iff unused</td>
    </tr>
    <tr>
      <th colspan="2">Optimizer states</th>
      <td colspan="2"><a class="text-ref" href="#no-optimiser-cost">None</a> in our setup</td>
    </tr>
    <tr>
      <th colspan="2">Activations</th>
      <td colspan="2"><a class="text-ref" href="#low-activation-cost">Negligible</a> in our setup</td>
    </tr>
  </tbody>
  <tfoot>
    <tr>
      <th colspan="2">Total</th>
      <td>
        8 bytes/param (fused), or<br>
        12 bytes/param (unfused)
      </td>
      <td>10 bytes/param</td>
    </tr>
  </tfoot>
</table>

<p>
<details><summary>*I believe that a compute copy is not taken for parameters whose layers <a href="https://pytorch.org/docs/stable/amp.html#cuda-ops-that-can-autocast-to-float32">autocast to single-precision</a></summary> (for example Embedding and LayerNorm – investigated in this <a href="https://github.com/scottlogic-alex/qlora/blob/memory-investigation/investigate_layernorm.py">minimal reproducer</a>).<br>
LayerNorm params are insignificant (comprising 0.00% of llama-7b’s params), but Embedding can be one of the biggest layers in an LLM. Embedding becomes a smaller proportion of parameters as a model is scaled up, so we can disregard it as far as the overall pattern is concerned.<br>
Consider this an upper bound, because the lifetime of a compute copy is a performance choice: it’s needed during both the forward and backward passes, so the fastest approach is to set aside enough memory to maintain permanent compute copies for all layers. But at minimum: one layer at a time can be recreated from the model’s float32 parameters, then freed after computation (and this work can be re-done during the backwards pass).</details>
<details><summary>**In PyTorch AMP: I believe the half-precision gradients are never materialized.</summary> They are instead reduced into the model’s float32 gradients immediately. Looking at <a href="https://gist.github.com/scottlogic-alex/9796d5583e5fe7672d7d829734c84c81">memory snapshots</a>: I see that the backwards pass for a Linear layer allocates a float32-sized buffer but no corresponding half-precision-sized buffer.</details>
<details><summary>***Transformer Math does not mention a "4 bytes/param master gradients" cost. I think this assumes a fused optimizer, such as DeepSpeed provides.</summary> I believe I see evidence of a master gradient cost <a href="https://github.com/microsoft/Megatron-DeepSpeed/blob/8e1ff895b5f646f2dc010ff5435aa183ec6d3f02/megatron/optimizer/optimizer.py#L618">in Megatron-DS</a>, and in contemporary DeepSpeed’s <a href="https://github.com/microsoft/DeepSpeed/blob/1d1a20c5a1dd980438393500f14d8dc188f6258a/deepspeed/runtime/fp16/unfused_optimizer.py#L224">unfused optimizer</a>.</details>
</p>

### Estimated cost of gradient accumulation

Gradient accumulation is a technique which computes the gradients for a minibatch, one microbatch at a time, summing each result (divided by the number of microbatches). This is typically done to save memory (at the expense of wall time), for a given minibatch size.
In distributed training, gradient accumulation can also be exploited to reduce the frequency with which synchronization of gradients is required, reducing the proportion of training time spent on communication overhead.

When running in mixed-precision: we expect gradient accumulation to cost:

<table class="lm-wireframe lm-lefthead lm-greyhead">
  <thead>
    <tr>
      <th>Megatron-DS,<br>DeepSpeed ZeRO</th>
      <th>PyTorch AMP</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0 or 4 bytes/param*</td>
      <td>0 or 4 bytes/param**</td>
    </tr>
  </tbody>
</table>

<p>
<details><summary>*Deepspeed offers two gradient accumulation modes, based on whether you accumulate and reduce gradients using the same datatype. If the datatypes match: you can use the same buffer for both, saving memory.</summary>
This is an unrealistic/legacy option though; it is better to reduce in half-precision (to send less data over the network in distributed training) and accumulate in float32 (accumulators benefit from increased precision). This requires the allocation of an additional float32 buffer, costing 4 bytes/param.</details>
<span class="reset-fontweight">**In PyTorch AMP: the model is held in float32, and hence its gradients are also float32. The backward pass accumulates the half-precision compute gradients into the model’s float32 gradient buffers, whether we do 1 microstep or many.</span>
</p>

### Estimated cost of distributed training

There are <a href="https://github.com/stas00/ml-engineering/tree/master/model-parallelism">many ways</a> to distribute training. Among the simplest is DDP (Distributed Data Parallel), which replicates the model across multiple GPUs. We wish to measure its costs here, because its ease-of-use makes it a good fit for hobbyist or small-scale training.

DDP doesn’t reduce the size of the model or optimizer states. It enables us to achieve bigger minibatch sizes, or give smaller microbatches to each GPU, reducing the per-GPU cost of activations. The experiment we’re doing here has already minimized the cost of activations. Hence we don’t expect DDP to save us memory, but we are interested in measuring the overhead incurred by the extra communication it necessitates.

We expect PyTorch DDP to cost <strong>4 bytes/param</strong>, as its distributed Reducer <a href="https://discuss.pytorch.org/t/memory-consumption-for-the-model-get-doubled-after-wrapped-with-ddp/130837/5">creates gradient buckets for each parameter</a>. This cost can be reduced by enabling <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"><code>gradient_as_bucket_view</code></a>.

The frameworks studied in Transformer Math can be distributed in more exotic ways than DDP, capable of sharding the model and optimizer states. DeepSpeed ZeRO allocates a ~477MiB bucket for <a href="https://github.com/microsoft/DeepSpeed/blob/a3926bbbf6d0025b5c6076a280e6b91ebd08aada/deepspeed/runtime/zero/stage_1_and_2.py#L663">allreduce operations</a>, a fixed cost that should not scale with model size.

## What we observed experimentally

We trained with batch-of-1, short sequences, and a stateless optimiser for two steps.
Where gradient accumulation was used: we ran each step for 3 microsteps.
For 7b models, we used <a href="https://huggingface.co/docs/transformers/v4.17.0/en/parallelism#naive-model-parallelism-vertical-and-pipeline-parallelism">naïve model parallelism</a> to vertically-shard costs over two 48GiB GPUs. This type of parallelism is non-performant and therefore unrealistic, but serves as a good way to simulate having one larger GPU. We were not able to measure DDP costs for this configuration, as we had no remaining GPUs over which to replicate the training.
Note also that when Nvidia markets the capacity of a GPU, they use the measure ‘GB’ but they actually mean GiB.

### Memory cost (MiB)

<table class="lm-wireframe lm-lefthead lm-greyhead">
  <thead>
    <tr>
      <th rowspan="4" class="bottom">Arch</th>
      <th rowspan="4" class="bottom">Model</th>
      <th rowspan="4" class="bottom">Param (b)</th>
      <th class="right">DDP</th>
      <th colspan="4" class="off">Off</th>
      <th colspan="4" class="on">On</th>
    </tr>
    <tr>
      <th class="right">AMP</th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
    </tr>
    <tr>
      <th class="right">GA</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
    </tr>
    <tr>
      <th/>
      <th colspan="8" class="center">Total memory used + reserved (GiB)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3">GPT-NeoX</th>
      <th>pythia-1.4b</th>
      <th>1.4</th>
      <th rowspan="5"/>
      <td class="datum">10.7</td>
      <td class="datum">11.1</td>
      <td class="datum">12.7</td>
      <td class="datum">13.8</td>
      <td class="datum">16.1</td>
      <td class="datum">16.6</td>
      <td class="datum">16.3</td>
      <td class="datum">19.5</td>
    </tr>
    <tr>
      <th>pythia-2.8b</th>
      <th>2.8</th>
      <td class="datum">21.0</td>
      <td class="datum">21.5</td>
      <td class="datum">25.0</td>
      <td class="datum">27.4</td>
      <td class="datum">31.8</td>
      <td class="datum">32.1</td>
      <td class="datum">32.4</td>
      <td class="datum">37.6</td>
    </tr>
    <tr>
      <th>pythia-6.9b</th>
      <th>6.9</th>
      <td class="datum">51.4</td>
      <td class="datum">53.1</td>
      <td class="datum">61.7</td>
      <td class="datum">67.8</td>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
    <tr>
      <th rowspan="3">Llama</th>
      <th>openllama-3b</th>
      <th>3.4</th>
      <td class="datum">25.9</td>
      <td class="datum">26.3</td>
      <td class="datum">29.8</td>
      <td class="datum">34.6</td>
      <td class="datum">39.5</td>
      <td class="datum">39.5</td>
      <td class="datum">40.2</td>
      <td class="oom">&gt;48</td>
    </tr>
    <tr>
      <th>llama-2-7b</th>
      <th>6.7</th>
      <td class="datum">50.5</td>
      <td class="datum">51.6</td>
      <td class="datum">57.4</td>
      <td class="datum">67.0</td>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
  </tbody>
</table>

We can see that almost none of these training configurations fit on an Nvidia RTX 4090 (24GiB), the largest consumer Nvidia GPU. It’s even a tight fit on a lot of server Nvidia GPUs (e.g. 40GiB and 48GiB). Bear in mind how these training configurations constitute a _lower bound_ (we aren’t paying for optimizer state, and our activations are unrealistically small).

We see that enabling _all_ of these training techniques is punitive, doubling our memory costs.

It’s clear why Nvidia’s top-end 80GiB GPUs are desirable, and why there is interest in <a href="https://www.mosaicml.com/blog/amd-mi250">training on AMD’s 128GiB GPUs</a>. We also see that realistically, the training of large models must rely upon the sharding of memory costs across GPUs.

Let’s now try to understand these costs as a function of model size.

### Memory cost (bytes/param)

Here we divide the memory usage by the number of model parameters.

<table class="lm-wireframe lm-lefthead lm-greyhead">
  <thead>
    <tr>
      <th rowspan="4" class="bottom">Arch</th>
      <th rowspan="4" class="bottom">Model</th>
      <th rowspan="4" class="bottom">Param (b)</th>
      <th class="right">DDP</th>
      <th colspan="4" class="off">Off</th>
      <th colspan="4" class="on">On</th>
    </tr>
    <tr>
      <th class="right">AMP</th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
      <th colspan="2" class="off">Off</th>
      <th colspan="2" class="on">On</th>
    </tr>
    <tr>
      <th class="right">GA</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
      <th class="off">Off</th>
      <th class="on">On</th>
    </tr>
    <tr>
      <th/>
      <th colspan="8" class="center">bytes/param</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3">GPT-NeoX</th>
      <th>pythia-1.4b</th>
      <th>1.4</th>
      <th rowspan="5"/>
      <td class="datum">8.1</td>
      <td class="datum">8.4</td>
      <td class="datum">9.6</td>
      <td class="datum">10.5</td>
      <td class="datum">12.2</td>
      <td class="datum">12.6</td>
      <td class="datum">12.3</td>
      <td class="datum">14.8</td>
    </tr>
    <tr>
      <th>pythia-2.8b</th>
      <th>2.8</th>
      <td class="datum">8.1</td>
      <td class="datum">8.3</td>
      <td class="datum">9.7</td>
      <td class="datum">10.6</td>
      <td class="datum">12.3</td>
      <td class="datum">12.4</td>
      <td class="datum">12.5</td>
      <td class="datum">14.5</td>
    </tr>
    <tr>
      <th>pythia-6.9b</th>
      <th>6.9</th>
      <td class="datum">8.0</td>
      <td class="datum">8.3</td>
      <td class="datum">9.7</td>
      <td class="datum">10.6</td>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
    <tr>
      <th rowspan="3">Llama</th>
      <th>openllama-3b</th>
      <th>3.4</th>
      <td class="datum">8.1</td>
      <td class="datum">8.2</td>
      <td class="datum">9.3</td>
      <td class="datum">10.9</td>
      <td class="datum">12.4</td>
      <td class="datum">12.4</td>
      <td class="datum">12.6</td>
      <td class="oom">&gt;15.0</td>
    </tr>
    <tr>
      <th>llama-2-7b</th>
      <th>6.7</th>
      <td class="datum">8.0</td>
      <td class="datum">8.2</td>
      <td class="datum">9.1</td>
      <td class="datum">10.7</td>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
      <td class="null"/>
    </tr>
  </tbody>
</table>

Remarkably, some very regular patterns emerge, scaling moreorless linearly with model size.

Our (strawman) baseline of "everything in float32" is indeed 8 bytes/param. This baseline should be low-overhead; it will incur less fragmentation, as it does not need to allocate as many temporary buffers as the other configurations.

Our estimate that "mixed-precision should cost 10 bytes/param" looks close (it’s actually slightly cheaper), but the theory "DDP costs 4 bytes/param more" only holds true when gradient accumulation is enabled. Something to dig into.

Let’s visualise these datapoints as relative overheads compared to our float32 baseline.